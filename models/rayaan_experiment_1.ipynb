{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "996c79e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Imports\n",
    "# -----------------------------------------------------------------------------\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# The transformers VideoMAE model & Feature Extractor\n",
    "from transformers import VideoMAEForVideoClassification, VideoMAEFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fcc6b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x287d1a13b30>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Config / Hyperparameters (small test run defaults)\n",
    "# -----------------------------------------------------------------------------\n",
    "ROOT_FRAMES = Path(r\"C:\\Users\\rayaa\\Downloads\\ucf_crime_v2\\ucf_crime_frames\")  # change if needed\n",
    "SPLITS = [\"Train\", \"Test\", \"Validation\"]\n",
    "NUM_FRAMES = 16          # clip length sampled uniformly across each video\n",
    "RESOLUTION = 224        # square resolution (you chose 224)\n",
    "BATCH_SIZE = 4          # small for test run; increase if you have GPU memory\n",
    "NUM_WORKERS = 0\n",
    "LR = 2e-4\n",
    "EPOCHS = 2              # minimal smoke test\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = None      # will be inferred from dataset\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59ba5fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Dataset: folder-of-frames where each video is a folder of PNG frames\n",
    "# We will uniformly sample NUM_FRAMES frames across the available frames.\n",
    "# Output tensor shape: (C, T, H, W) where C=3, T=NUM_FRAMES\n",
    "# -----------------------------------------------------------------------------\n",
    "class UCFFolderFramesDataset(Dataset):\n",
    "    def __init__(self, root: Path, split: str = \"Train\", num_frames: int = 16, resolution: int = 224, class_names: List[str] = None):\n",
    "        self.root = Path(root) / split\n",
    "        self.num_frames = num_frames\n",
    "        self.resolution = resolution\n",
    "        self.samples = []  # list of tuples (video_folder_path, class_idx)\n",
    "\n",
    "        # gather classes\n",
    "        if class_names is None:\n",
    "            self.classes = sorted([p.name for p in self.root.iterdir() if p.is_dir()])\n",
    "        else:\n",
    "            self.classes = class_names\n",
    "\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
    "\n",
    "        # iterate classes and videos\n",
    "        for class_name in self.classes:\n",
    "            class_dir = self.root / class_name\n",
    "            if not class_dir.exists():\n",
    "                continue\n",
    "            for video_folder in class_dir.iterdir():\n",
    "                if video_folder.is_dir():\n",
    "                    # count image files (common extensions)\n",
    "                    frame_files = sorted([p for p in video_folder.iterdir() if p.suffix.lower() in [\".png\"]])\n",
    "                    if len(frame_files) < 2:\n",
    "                        continue\n",
    "                    self.samples.append((video_folder, self.class_to_idx[class_name]))\n",
    "\n",
    "        # transforms for each frame\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((self.resolution, self.resolution)),\n",
    "            T.ToTensor(),  # produces [C, H, W] float in [0,1]\n",
    "            # we will not normalize here, as we'll rely on the VideoMAE feature extractor if necessary\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _uniform_sample_indices(self, num_total: int) -> List[int]:\n",
    "        \"\"\"Return list of indices (length self.num_frames) uniformly sampled across [0, num_total-1].\n",
    "        If num_total < num_frames, we will pad by repeating the last frame.\n",
    "        \"\"\"\n",
    "        if num_total <= 0:\n",
    "            raise ValueError(\"video has no frames\")\n",
    "        if num_total >= self.num_frames:\n",
    "            # linspace indices rounded to int\n",
    "            indices = np.linspace(0, num_total - 1, num=self.num_frames, dtype=int).tolist()\n",
    "        else:\n",
    "            # take all frames and repeat last\n",
    "            indices = list(range(num_total))\n",
    "            while len(indices) < self.num_frames:\n",
    "                indices.append(num_total - 1)\n",
    "        return indices\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        video_folder, class_idx = self.samples[idx]\n",
    "        frame_files = sorted([p for p in video_folder.iterdir() if p.suffix.lower() in [\".png\"]])\n",
    "        num_total = len(frame_files)\n",
    "        indices = self._uniform_sample_indices(num_total)\n",
    "\n",
    "        frames = []\n",
    "        for i in indices:\n",
    "            img = Image.open(frame_files[i]).convert(\"RGB\")\n",
    "            frames.append(self.transform(img))  # [C,H,W]\n",
    "\n",
    "        # stack frames -> shape [T, C, H, W]\n",
    "        frames = torch.stack(frames, dim=0)\n",
    "        # reorder to [C, T, H, W] which VideoMAE expects\n",
    "        # frames = frames.permute(1, 0, 2, 3).contiguous()\n",
    "\n",
    "        return frames, class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d56c050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Utility: build datasets + dataloaders (small subset mode for quick test)\n",
    "# -----------------------------------------------------------------------------\n",
    "def build_dataloaders(root_frames: Path, num_frames: int = NUM_FRAMES, resolution: int = RESOLUTION, batch_size: int = BATCH_SIZE, small_run: bool = True):\n",
    "    train_root = root_frames / \"Train\"\n",
    "    # infer classes from train\n",
    "    classes = sorted([p.name for p in train_root.iterdir() if p.is_dir()])\n",
    "    print(f\"Detected classes ({len(classes)}): {classes}\")\n",
    "\n",
    "    train_ds = UCFFolderFramesDataset(root_frames, split=\"Train\", num_frames=num_frames, resolution=resolution, class_names=classes)\n",
    "    test_ds = UCFFolderFramesDataset(root_frames, split=\"Test\", num_frames=num_frames, resolution=resolution, class_names=classes)\n",
    "\n",
    "    # For a minimal smoke test, use small subsets\n",
    "    if small_run:\n",
    "        # pick at most 100 train samples, 50 test samples (or fewer if dataset smaller)\n",
    "        n_train = min(len(train_ds), 100)\n",
    "        n_test = min(len(test_ds), 50)\n",
    "        train_ds, _ = random_split(train_ds, [n_train, max(0, len(train_ds) - n_train)])\n",
    "        test_ds, _ = random_split(test_ds, [n_test, max(0, len(test_ds) - n_test)])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    return train_loader, test_loader, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4fcb6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Model setup: VideoMAE from Hugging Face\n",
    "# Note: we will use the feature extractor to normalize images appropriately\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def build_model(num_labels: int):\n",
    "    # load feature extractor to handle normalization / resizing guidance\n",
    "    feature_extractor = VideoMAEFeatureExtractor()\n",
    "\n",
    "    # instantiate the model for video classification; use a small pretrained checkpoint if available\n",
    "    model = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base\", num_labels=num_labels)\n",
    "    # If the above checkpoint isn't available locally/internet, fallback to random initialization\n",
    "    # model = VideoMAEForVideoClassification.from_config(config)\n",
    "\n",
    "    return model, feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "558d7918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Training + evaluation loops (minimal)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Train\", leave=False):\n",
    "        videos, labels = batch  # videos: [B, C, T, H, W]\n",
    "        videos = videos.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pixel_values=videos, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * videos.size(0)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += videos.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total if total > 0 else 0.0\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Eval\", leave=False):\n",
    "            videos, labels = batch\n",
    "            videos = videos.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(pixel_values=videos, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_loss += loss.item() * videos.size(0)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += videos.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total if total > 0 else 0.0\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36e221fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Main: build dataloaders, model, train and evaluate (small run)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def main_small_run():\n",
    "    train_loader, test_loader, classes = build_dataloaders(ROOT_FRAMES, num_frames=NUM_FRAMES, resolution=RESOLUTION, batch_size=BATCH_SIZE, small_run=True)\n",
    "    num_labels = len(classes)\n",
    "\n",
    "    model, feature_extractor = build_model(num_labels)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "    print(f\"Starting training on device={DEVICE} with {num_labels} classes\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, DEVICE)\n",
    "        print(f\" Train loss: {train_loss:.4f}  acc: {train_acc:.4f}\")\n",
    "\n",
    "        val_loss, val_acc = evaluate(model, test_loader, DEVICE)\n",
    "        print(f\" Eval loss:  {val_loss:.4f}  acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save a small checkpoint\n",
    "    ckpt_path = Path(\"videomae_small_run.pth\")\n",
    "    torch.save(model.state_dict(), ckpt_path)\n",
    "    print(f\"Saved checkpoint to {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d099378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected classes (14): ['Abuse', 'Arrest', 'Arson', 'Assault', 'Burglary', 'Explosion', 'Fighting', 'NormalVideos', 'RoadAccidents', 'Robbery', 'Shooting', 'Shoplifting', 'Stealing', 'Vandalism']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on device=cuda with 14 classes\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157f4e38ec9349f2876a3e4ac5f68fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train loss: 2.7330  acc: 0.1100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead8b99013104c20a97a263296a0e532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval loss:  2.8125  acc: 0.1000\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e38fb6b97240908a601b342fabb553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train loss: 2.5954  acc: 0.1100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147a7f64b4be494d9bca8727b094c07d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval loss:  2.8855  acc: 0.0400\n",
      "Saved checkpoint to videomae_small_run.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Run the small test run\n",
    "# -----------------------------------------------------------------------------\n",
    "main_small_run()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Notes / next steps\n",
    "# - The script uses MCG-NJU/videomae-base pretrained weights as an example; change if you prefer\n",
    "# - For large scale training: use gradient accumulation, mixed precision, more workers, and a scheduler\n",
    "# - Consider using the VideoMAEFeatureExtractor to pre-process frames into pixel_values; here we rely on basic tensor transforms.\n",
    "# - If you encounter shape mismatches, make sure the model expects video shape: (batch, channels, frames, height, width)\n",
    "# - For reproducibility, set deterministic flags and seeds for data loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c2d06235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected classes (14): ['Abuse', 'Arrest', 'Arson', 'Assault', 'Burglary', 'Explosion', 'Fighting', 'NormalVideos', 'RoadAccidents', 'Robbery', 'Shooting', 'Shoplifting', 'Stealing', 'Vandalism']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on device=cuda with 14 classes\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9f7ac14ff940bdb80304ff70b376e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train loss: 2.7139  acc: 0.0768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e3988ffe0a44858bf7756203ddfb7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval loss:  2.6248  acc: 0.1429\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c69d901d3aad4574aca2ce7f19bc039f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train loss: 2.6564  acc: 0.0929\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e93b5a69a31447fa2ce49fdee22f326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval loss:  2.6280  acc: 0.1143\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e34582d89940419aa6e817deec3045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train loss: 2.6509  acc: 0.0911\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9dda05fd8944bf8b41a0d5df19afe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval loss:  2.6178  acc: 0.0857\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f69d3bdde54e2ba8ede365b958a775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train loss: 2.6156  acc: 0.1071\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15063d105c59496eafdb46f9238efe0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval loss:  2.5830  acc: 0.1286\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91879b2f79d24908bee38471a42229b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train loss: 2.5868  acc: 0.1125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a22795285043ea8234f43d11c4c4a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval loss:  2.5962  acc: 0.1286\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91efc95006804a9abd6d39430db9c114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train loss: 2.5856  acc: 0.1250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c4134c8a244e769b3806abb94b7dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval loss:  2.5851  acc: 0.1000\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40792582a5544781a6c37b187d57c739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train loss: 2.5504  acc: 0.1321\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d8ca80771a4a06844e325b7fb33232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval loss:  2.6460  acc: 0.0429\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f3f480e8ae4754b2c9295c8a53cbe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train loss: 2.5300  acc: 0.1357\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29499cbf44504fb1b376f1e9949b14ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval loss:  2.5878  acc: 0.2000\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf515e930544a22b131d08d6866effd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train loss: 2.5123  acc: 0.1375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7705c3ea92514537b3429840dd3b7d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval loss:  2.5962  acc: 0.1000\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b7e32f72dc4cc9aaa139985d5ef246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train loss: 2.4724  acc: 0.1464\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4983ae9feee54c90912179ff5afe7426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval loss:  2.7030  acc: 0.0857\n",
      "Saved checkpoint to videomae_small_run.pth\n"
     ]
    }
   ],
   "source": [
    "# VideoMAE training notebook (filled)\n",
    "# This notebook implements a minimal VideoMAE training + evaluation pipeline\n",
    "# using the frame-extracted UCF Crime dataset (\"ucf_crime_frames/\").\n",
    "#\n",
    "# Choices you requested:\n",
    "# 1) Frame sampling: Uniform sample N frames evenly across the video\n",
    "# 2) Input resolution: 224x224\n",
    "# 3) Run mode: Minimal test run (small subset) to verify training loop\n",
    "\n",
    "# Requirements\n",
    "# - torch, torchvision\n",
    "# - transformers (for VideoMAE model and feature extractor)\\# - tqdm\n",
    "# - pillow\n",
    "# - optionally accelerate for fp16 (not used here)\n",
    "\n",
    "# If any of these are missing, install via pip, for example:\n",
    "# !pip install torch torchvision transformers tqdm pillow\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Imports\n",
    "# -----------------------------------------------------------------------------\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# The transformers VideoMAE model & Feature Extractor\n",
    "from transformers import VideoMAEForVideoClassification, VideoMAEFeatureExtractor\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Config / Hyperparameters (small test run defaults)\n",
    "# -----------------------------------------------------------------------------\n",
    "ROOT_FRAMES = Path(r\"C:\\Users\\rayaa\\Downloads\\ucf_crime_v2\\ucf_crime_frames\")\n",
    "SPLITS = [\"Train\", \"Test\", \"Validation\"]\n",
    "NUM_FRAMES = 16          # clip length sampled uniformly across each video\n",
    "RESOLUTION = 224        # square resolution (you chose 224)\n",
    "BATCH_SIZE = 4          # small for test run; increase if you have GPU memory\n",
    "# IMPORTANT FIX for Windows multiprocessing crash during DataLoader\n",
    "# Windows + PIL image loading often crashes worker processes when using multiple workers.\n",
    "# Set NUM_WORKERS = 0 to avoid multiprocessing issues.\n",
    "NUM_WORKERS = 0\n",
    "LR = 2e-4\n",
    "EPOCHS = 10              # minimal smoke test\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = None      # will be inferred from dataset\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Dataset: folder-of-frames where each video is a folder of PNG frames\n",
    "# We will uniformly sample NUM_FRAMES frames across the available frames.\n",
    "# Output tensor shape: (C, T, H, W) where C=3, T=NUM_FRAMES\n",
    "# -----------------------------------------------------------------------------\n",
    "class UCFFolderFramesDataset(Dataset):\n",
    "    def __init__(self, root: Path, split: str = \"Train\", num_frames: int = 16, resolution: int = 224, class_names: List[str] = None):\n",
    "        self.root = Path(root) / split\n",
    "        self.num_frames = num_frames\n",
    "        self.resolution = resolution\n",
    "        self.samples = []  # list of tuples (video_folder_path, class_idx)\n",
    "\n",
    "        # gather classes\n",
    "        if class_names is None:\n",
    "            self.classes = sorted([p.name for p in self.root.iterdir() if p.is_dir()])\n",
    "        else:\n",
    "            self.classes = class_names\n",
    "\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
    "\n",
    "        # iterate classes and videos\n",
    "        for class_name in self.classes:\n",
    "            class_dir = self.root / class_name\n",
    "            if not class_dir.exists():\n",
    "                continue\n",
    "            for video_folder in class_dir.iterdir():\n",
    "                if video_folder.is_dir():\n",
    "                    # count image files (common extensions)\n",
    "                    frame_files = sorted([p for p in video_folder.iterdir() if p.suffix.lower() in [\".png\", \".jpg\", \".jpeg\"]])\n",
    "                    if len(frame_files) < 2:\n",
    "                        continue\n",
    "                    self.samples.append((video_folder, self.class_to_idx[class_name]))\n",
    "\n",
    "        # transforms for each frame\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((self.resolution, self.resolution)),\n",
    "            T.ToTensor(),  # produces [C, H, W] float in [0,1]\n",
    "            # we will not normalize here, as we'll rely on the VideoMAE feature extractor if necessary\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _uniform_sample_indices(self, num_total: int) -> List[int]:\n",
    "        \"\"\"Return list of indices (length self.num_frames) uniformly sampled across [0, num_total-1].\n",
    "        If num_total < num_frames, we will pad by repeating the last frame.\n",
    "        \"\"\"\n",
    "        if num_total <= 0:\n",
    "            raise ValueError(\"video has no frames\")\n",
    "        if num_total >= self.num_frames:\n",
    "            # linspace indices rounded to int\n",
    "            indices = np.linspace(0, num_total - 1, num=self.num_frames, dtype=int).tolist()\n",
    "        else:\n",
    "            # take all frames and repeat last\n",
    "            indices = list(range(num_total))\n",
    "            while len(indices) < self.num_frames:\n",
    "                indices.append(num_total - 1)\n",
    "        return indices\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        video_folder, class_idx = self.samples[idx]\n",
    "        frame_files = sorted([p for p in video_folder.iterdir() if p.suffix.lower() in [\".png\", \".jpg\", \".jpeg\"]])\n",
    "        num_total = len(frame_files)\n",
    "        indices = self._uniform_sample_indices(num_total)\n",
    "\n",
    "        frames = []\n",
    "        for i in indices:\n",
    "            img = Image.open(frame_files[i]).convert(\"RGB\")\n",
    "            frames.append(self.transform(img))  # [C,H,W]\n",
    "\n",
    "        # stack frames -> shape [T, C, H, W]\n",
    "        frames = torch.stack(frames, dim=0)\n",
    "\n",
    "        return frames, class_idx\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Utility: build datasets + dataloaders (small subset mode for quick test)\n",
    "# -----------------------------------------------------------------------------\n",
    "def build_dataloaders(root_frames: Path, num_frames: int = NUM_FRAMES, resolution: int = RESOLUTION, batch_size: int = BATCH_SIZE, small_run: bool = True):\n",
    "    train_root = root_frames / \"Train\"\n",
    "    # infer classes from train\n",
    "    classes = sorted([p.name for p in train_root.iterdir() if p.is_dir()])\n",
    "    print(f\"Detected classes ({len(classes)}): {classes}\")\n",
    "\n",
    "    train_ds = UCFFolderFramesDataset(root_frames, split=\"Train\", num_frames=num_frames, resolution=resolution, class_names=classes)\n",
    "    test_ds = UCFFolderFramesDataset(root_frames, split=\"Test\", num_frames=num_frames, resolution=resolution, class_names=classes)\n",
    "\n",
    "    # For a minimal smoke test, use small subsets\n",
    "    if small_run:\n",
    "        # pick at most 100 train samples, 50 test samples (or fewer if dataset smaller)\n",
    "        n_train = min(len(train_ds), 100)\n",
    "        n_test = min(len(test_ds), 50)\n",
    "        train_ds, _ = random_split(train_ds, [n_train, max(0, len(train_ds) - n_train)])\n",
    "        test_ds, _ = random_split(test_ds, [n_test, max(0, len(test_ds) - n_test)])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    return train_loader, test_loader, classes\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Model setup: VideoMAE from Hugging Face\n",
    "# Note: we will use the feature extractor to normalize images appropriately\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def build_model(num_labels: int):\n",
    "    # load feature extractor to handle normalization / resizing guidance\n",
    "    feature_extractor = VideoMAEFeatureExtractor()\n",
    "\n",
    "    # instantiate the model for video classification; use a small pretrained checkpoint if available\n",
    "    model = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base\", num_labels=num_labels)\n",
    "    # If the above checkpoint isn't available locally/internet, fallback to random initialization\n",
    "    # model = VideoMAEForVideoClassification.from_config(config)\n",
    "\n",
    "    return model, feature_extractor\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Training + evaluation loops (minimal)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Train\", leave=False):\n",
    "        videos, labels = batch  # videos: [B, C, T, H, W]\n",
    "        videos = videos.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pixel_values=videos, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * videos.size(0)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += videos.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total if total > 0 else 0.0\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Eval\", leave=False):\n",
    "            videos, labels = batch\n",
    "            videos = videos.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(pixel_values=videos, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_loss += loss.item() * videos.size(0)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += videos.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total if total > 0 else 0.0\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    return avg_loss, acc\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main: build dataloaders, model, train and evaluate (small run)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def main_small_run():\n",
    "    train_loader, test_loader, classes = build_dataloaders(ROOT_FRAMES, num_frames=NUM_FRAMES, resolution=RESOLUTION, batch_size=BATCH_SIZE, small_run=False)\n",
    "    num_labels = len(classes)\n",
    "\n",
    "    model, feature_extractor = build_model(num_labels)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "    print(f\"Starting training on device={DEVICE} with {num_labels} classes\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, DEVICE)\n",
    "        print(f\" Train loss: {train_loss:.4f}  acc: {train_acc:.4f}\")\n",
    "\n",
    "        val_loss, val_acc = evaluate(model, test_loader, DEVICE)\n",
    "        print(f\" Eval loss:  {val_loss:.4f}  acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save a small checkpoint\n",
    "    ckpt_path = Path(\"videomae_small_run.pth\")\n",
    "    torch.save(model.state_dict(), ckpt_path)\n",
    "    print(f\"Saved checkpoint to {ckpt_path}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Run the small test run\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main_small_run()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Notes / next steps\n",
    "# - The script uses MCG-NJU/videomae-base pretrained weights as an example; change if you prefer\n",
    "# - For large scale training: use gradient accumulation, mixed precision, more workers, and a scheduler\n",
    "# - Consider using the VideoMAEFeatureExtractor to pre-process frames into pixel_values; here we rely on basic tensor transforms.\n",
    "# - If you encounter shape mismatches, make sure the model expects video shape: (batch, channels, frames, height, width)\n",
    "# - For reproducibility, set deterministic flags and seeds for data loading.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_12_8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
