{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b13fddf",
   "metadata": {},
   "source": [
    "### Imports\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd8193e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rayaa\\anaconda3\\envs\\cuda_12_8_fr\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import AutoImageProcessor, VideoMAEForVideoClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49e58aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorchvideo.data\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e116f",
   "metadata": {},
   "source": [
    "### Data Directories\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27ca541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = r\"C:\\Users\\rayaa\\Downloads\\ucf_crime_v2\\Train\"\n",
    "test_root = r\"C:\\Users\\rayaa\\Downloads\\ucf_crime_v2\\Test\"\n",
    "val_root = r\"C:\\Users\\rayaa\\Downloads\\ucf_crime_v2\\Validation\"\n",
    "\n",
    "output_dir = r\"C:\\Users\\rayaa\\Downloads\\ucf_crime_v2\\output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebe38a1",
   "metadata": {},
   "source": [
    "### Verify Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdfc73f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abuse: 40 videos\n",
      "Arrest: 40 videos\n",
      "Arson: 40 videos\n",
      "Assault: 40 videos\n",
      "Burglary: 80 videos\n",
      "Explosion: 40 videos\n",
      "Fighting: 40 videos\n",
      "NormalVideos: 170 videos\n",
      "RoadAccidents: 120 videos\n",
      "Robbery: 120 videos\n",
      "Shooting: 40 videos\n",
      "Shoplifting: 40 videos\n",
      "Stealing: 80 videos\n",
      "Vandalism: 40 videos\n",
      "Abuse: 5 videos\n",
      "Arrest: 5 videos\n",
      "Arson: 5 videos\n",
      "Assault: 5 videos\n",
      "Burglary: 10 videos\n",
      "Explosion: 5 videos\n",
      "Fighting: 5 videos\n",
      "NormalVideos: 150 videos\n",
      "RoadAccidents: 15 videos\n",
      "Robbery: 15 videos\n",
      "Shooting: 5 videos\n",
      "Shoplifting: 5 videos\n",
      "Stealing: 10 videos\n",
      "Vandalism: 5 videos\n",
      "Abuse: 5 videos\n",
      "Arrest: 5 videos\n",
      "Arson: 5 videos\n",
      "Assault: 5 videos\n",
      "Burglary: 10 videos\n",
      "Explosion: 5 videos\n",
      "Fighting: 5 videos\n",
      "NormalVideos: 19 videos\n",
      "RoadAccidents: 15 videos\n",
      "Robbery: 15 videos\n",
      "Shooting: 5 videos\n",
      "Shoplifting: 5 videos\n",
      "Stealing: 10 videos\n",
      "Vandalism: 5 videos\n",
      "\n",
      "Total Train videos: 930\n",
      "Total Test videos: 245\n",
      "Total Validation videos: 114\n"
     ]
    }
   ],
   "source": [
    "def count_videos(root_dir):\n",
    "    total = 0\n",
    "    for cls in os.listdir(root_dir):\n",
    "        cls_path = os.path.join(root_dir, cls)\n",
    "        if not os.path.isdir(cls_path):\n",
    "            continue\n",
    "        # Count subdirectories (each subdirectory is a video folder)\n",
    "        num_videos = sum(\n",
    "            1 for file in os.listdir(cls_path)\n",
    "            if file.lower().endswith('.mp4')\n",
    "        )\n",
    "        print(f\"{cls}: {num_videos} videos\")\n",
    "        total += num_videos\n",
    "    return total\n",
    "\n",
    "train_count = count_videos(train_root)\n",
    "test_count = count_videos(test_root)\n",
    "val_count = count_videos(val_root)\n",
    "\n",
    "print(f\"\\nTotal Train videos: {train_count}\")\n",
    "print(f\"Total Test videos: {test_count}\")\n",
    "print(f\"Total Validation videos: {val_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fed0b61",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4544857e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Abuse': 0, 'Arrest': 1, 'Arson': 2, 'Assault': 3, 'Burglary': 4, 'Explosion': 5, 'Fighting': 6, 'NormalVideos': 7, 'RoadAccidents': 8, 'Robbery': 9, 'Shooting': 10, 'Shoplifting': 11, 'Stealing': 12, 'Vandalism': 13}\n"
     ]
    }
   ],
   "source": [
    "class_labels = sorted(os.listdir(train_root))\n",
    "num_classes = len(class_labels)\n",
    "label2id = {label: idx for idx, label in enumerate(class_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4f5dac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MapAndPath(root, label2id):\n",
    "    videopaths = []\n",
    "\n",
    "    for clss in sorted(os.listdir(root)):\n",
    "        class_dir = os.path.join(root, clss)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "\n",
    "        for file in os.listdir(class_dir):\n",
    "            if file.lower().endswith('.mp4'):\n",
    "                filepath = os.path.join(class_dir, file)\n",
    "                label_id = label2id[clss]\n",
    "                videopaths.append((filepath, {\"label\": label_id, \"video_path\": filepath}))\n",
    "                \n",
    "    return videopaths\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f014eec",
   "metadata": {},
   "source": [
    "### Dataset Class\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c33c80ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SafeLabeledVideoDataset(Dataset):\n",
    "    def __init__(self, labeled_video_paths, transform=None, num_frames=16, frame_size=224):\n",
    "        self.labeled_video_paths = labeled_video_paths\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "        self.frame_size = frame_size\n",
    "        self.length = len(labeled_video_paths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def _load_video_cv2(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(\"Could not open video\")\n",
    "\n",
    "        frames = []\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n",
    "\n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, (self.frame_size, self.frame_size))\n",
    "            # Store as (H, W, C) first\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if len(frames) == 0:\n",
    "            raise ValueError(\"No frames extracted\")\n",
    "\n",
    "        # Convert to numpy array and normalize\n",
    "        video_array = np.array(frames).astype(np.float32) / 255.0  # (T, H, W, C)\n",
    "        # Convert to tensor and permute to (C, T, H, W) for transforms\n",
    "        video_tensor = torch.from_numpy(video_array).permute(3, 0, 1, 2)  # (C, T, H, W)\n",
    "        \n",
    "        return video_tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, metadata = self.labeled_video_paths[idx]\n",
    "        try:\n",
    "            video_tensor = self._load_video_cv2(video_path)\n",
    "            print(f\"After load: {video_tensor.shape}\")\n",
    "\n",
    "            if self.transform:\n",
    "                video_dict = {\"video\": video_tensor}\n",
    "                print(f\"Before transform: {video_dict['video'].shape}\")\n",
    "                video_dict = self.transform(video_dict)\n",
    "                print(f\"After transform: {video_dict['video'].shape}\")\n",
    "                video_tensor = video_dict[\"video\"]\n",
    "\n",
    "            return {\n",
    "                \"video\": video_tensor,\n",
    "                \"label\": metadata[\"label\"],\n",
    "                \"video_path\": video_path,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[LOAD FAILED] {video_path} | Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            # Create dummy tensor in correct format\n",
    "            dummy = torch.zeros(3, self.num_frames, self.frame_size, self.frame_size)\n",
    "            return {\"video\": dummy, \"label\": metadata[\"label\"], \"video_path\": video_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c5f2e2",
   "metadata": {},
   "source": [
    "### Load VideoMAE\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50397355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers.utils as utils\n",
    "utils.is_torch_available = lambda: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0029e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"MCG-NJU/videomae-base\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = VideoMAEForVideoClassification.from_pretrained(model_name, num_labels=num_classes, label2id=label2id, id2label=id2label, ignore_mismatched_sizes=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d94063",
   "metadata": {},
   "source": [
    "### Data Transforms & Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab7dea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    height = width = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    height = image_processor.size[\"height\"]\n",
    "    width = image_processor.size[\"width\"]\n",
    "resize_to = (height, width)\n",
    "\n",
    "num_frames_to_sample = model.config.num_frames\n",
    "sample_rate = 4\n",
    "fps = 30\n",
    "clip_duration = num_frames_to_sample * sample_rate / fps\n",
    "\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    RandomShortSideScale(min_size=256, max_size=320),\n",
    "                    RandomCrop(resize_to),\n",
    "                    RandomHorizontalFlip(p=0.5),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    Resize(resize_to),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbaad712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 930, Test: 245, Val: 114\n"
     ]
    }
   ],
   "source": [
    "train_paths = MapAndPath(train_root, label2id)\n",
    "val_paths = MapAndPath(val_root, label2id)\n",
    "test_paths = MapAndPath(test_root, label2id)\n",
    "\n",
    "print(f\"Train: {len(train_paths)}, Test: {len(test_paths)}, Val: {len(val_paths)}\")\n",
    "\n",
    "train_dataset = SafeLabeledVideoDataset(\n",
    "    labeled_video_paths=train_paths,\n",
    "    transform=train_transform\n",
    "    \n",
    ")\n",
    "\n",
    "test_dataset = SafeLabeledVideoDataset(\n",
    "    labeled_video_paths=test_paths,\n",
    "    transform=val_transform\n",
    "    \n",
    ")\n",
    "\n",
    "val_dataset = SafeLabeledVideoDataset(\n",
    "    labeled_video_paths=val_paths,\n",
    "    transform=val_transform\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2dcf2d",
   "metadata": {},
   "source": [
    "### Trainere (NO TRAINING)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a6e8697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rayaa\\AppData\\Local\\Temp\\ipykernel_17788\\1876494569.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=preds, references=eval_pred.label_ids)\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([ex[\"video\"] for ex in examples]) \n",
    "    # Ensure the tensor is in the right format for VideoMAE\n",
    "    # VideoMAE expects: (batch_size, num_frames, num_channels, height, width)\n",
    "    pixel_values = pixel_values.permute(0, 2, 1, 3, 4)  # from (B, C, T, H, W) to (B, T, C, H, W)\n",
    "    \n",
    "    labels = torch.tensor([ex[\"label\"] for ex in examples])\n",
    "    paths = [ex.get(\"video_path\", \"unknown\") for ex in examples]\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels, \"video_path\": paths}\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 3\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=str(output_dir),\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=10,\n",
    "        learning_rate=5e-5,\n",
    "        warmup_ratio=0.1,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        num_train_epochs=num_epochs,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        push_to_hub=False,\n",
    "        remove_unused_columns=False,\n",
    "        max_steps=(len(train_dataset) // batch_size) * num_epochs,\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=image_processor,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=collate_fn,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1da289e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model num_frames: 16\n",
      "Model image size: 224\n",
      "Model num channels: 3\n"
     ]
    }
   ],
   "source": [
    "# Check what the model expects\n",
    "print(f\"Model num_frames: {model.config.num_frames}\")\n",
    "print(f\"Model image size: {model.config.image_size}\")\n",
    "print(f\"Model num channels: {model.config.num_channels}\")\n",
    "\n",
    "# Make sure your transforms match the model config\n",
    "num_frames_to_sample = model.config.num_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb88a6ab",
   "metadata": {},
   "source": [
    "### Testing Loop\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46a1b4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/82 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|          | 1/82 [00:02<03:01,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   2%|▏         | 2/82 [00:04<02:42,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   4%|▎         | 3/82 [00:06<02:39,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   5%|▍         | 4/82 [00:07<02:29,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   6%|▌         | 5/82 [00:09<02:24,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   7%|▋         | 6/82 [00:11<02:31,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   9%|▊         | 7/82 [00:13<02:29,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  10%|▉         | 8/82 [00:16<02:33,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  11%|█         | 9/82 [00:18<02:26,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  12%|█▏        | 10/82 [00:20<02:25,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  13%|█▎        | 11/82 [00:22<02:24,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  15%|█▍        | 12/82 [00:24<02:23,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  16%|█▌        | 13/82 [00:26<02:23,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  17%|█▋        | 14/82 [00:28<02:20,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  18%|█▊        | 15/82 [00:30<02:14,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  20%|█▉        | 16/82 [00:32<02:13,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  21%|██        | 17/82 [00:34<02:09,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  22%|██▏       | 18/82 [00:36<02:10,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  23%|██▎       | 19/82 [00:38<02:07,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  24%|██▍       | 20/82 [00:40<02:04,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  26%|██▌       | 21/82 [00:42<02:01,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  27%|██▋       | 22/82 [00:44<01:57,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  28%|██▊       | 23/82 [00:46<01:56,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  29%|██▉       | 24/82 [00:48<01:54,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  30%|███       | 25/82 [00:50<01:52,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  32%|███▏      | 26/82 [00:52<01:48,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n",
      "After load: torch.Size([3, 16, 224, 224])\n",
      "Before transform: torch.Size([3, 16, 224, 224])\n",
      "After transform: torch.Size([3, 16, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  32%|███▏      | 26/82 [00:54<01:56,  2.08s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m labels = batch[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m].to(device)\n\u001b[32m     17\u001b[39m outputs = model(**inputs)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m preds = \u001b[43moutputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.tolist()\n\u001b[32m     19\u001b[39m all_preds.extend(preds)\n\u001b[32m     20\u001b[39m all_labels.extend(labels.cpu().tolist())\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds, all_labels, failed_paths = [], [], []\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        try:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k not in [\"labels\", \"video_path\"]}\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(**inputs)\n",
    "            preds = outputs.logits.argmax(dim=1).cpu().tolist()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "        except Exception as e:\n",
    "            print(f\"Batch failed: {batch.get('video_path', [])} | {e}\")\n",
    "            failed_paths.extend(batch.get(\"video_path\", []))\n",
    "            # Add dummy preds to keep length\n",
    "            all_preds.extend([0] * len(labels))\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "print(f\"Failed videos: {len(failed_paths)}\")\n",
    "if failed_paths:\n",
    "    print(\"First 5 failed paths:\", failed_paths[:5])\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=np.arange(num_classes))\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=class_labels,\n",
    "    yticklabels=class_labels,\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Test Set Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "cm_path = output_dir / \"confusion_matrix.png\"\n",
    "plt.savefig(cm_path)\n",
    "plt.close()\n",
    "print(f\"Confusion matrix saved to {cm_path}\")\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(\n",
    "    all_labels, all_preds, target_names=class_labels, digits=4\n",
    ")\n",
    "report_path = output_dir / \"classification_report.txt\"\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(report)\n",
    "print(\"\\nClassification report:\\n\", report)\n",
    "print(f\"Report saved to {report_path}\")\n",
    "\n",
    "acc = np.trace(cm) / np.sum(cm)\n",
    "print(f\"\\nOverall Test Accuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_12_8_fr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
