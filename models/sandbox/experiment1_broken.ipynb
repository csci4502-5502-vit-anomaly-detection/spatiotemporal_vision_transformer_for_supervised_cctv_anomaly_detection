{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b13fddf",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd8193e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Brandon\\anaconda3\\envs\\ucf_crime_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import AutoImageProcessor, VideoMAEForVideoClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad66094",
   "metadata": {},
   "source": [
    "to use pytorchvideo.transforms, i installed torch version 2.0.1, and torchvision version 0.15.2\n",
    "\n",
    "newer versions of torchvision remove necessary modules for pytorchvideo\n",
    "\n",
    "pre 2.0 numpy was also needed to use numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49e58aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Brandon\\anaconda3\\envs\\ucf_crime_env\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pytorchvideo.data\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e116f",
   "metadata": {},
   "source": [
    "set root and output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27ca541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = r\"C:\\Users\\Brandon\\Documents\\DM_Project\\ucf_crime_v2\\Train\"\n",
    "test_root = r\"C:\\Users\\Brandon\\Documents\\DM_Project\\ucf_crime_v2\\Test\"\n",
    "val_root = r\"C:\\Users\\Brandon\\Documents\\DM_Project\\ucf_crime_v2\\Validation\"\n",
    "\n",
    "output_dir = r\"C:\\Users\\Brandon\\Documents\\DM_Project\\output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebe38a1",
   "metadata": {},
   "source": [
    "Verify Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdfc73f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abuse: 40 videos\n",
      "Arrest: 40 videos\n",
      "Arson: 40 videos\n",
      "Assault: 40 videos\n",
      "Burglary: 80 videos\n",
      "Explosion: 40 videos\n",
      "Fighting: 40 videos\n",
      "NormalVideos: 170 videos\n",
      "RoadAccidents: 120 videos\n",
      "Robbery: 120 videos\n",
      "Shooting: 40 videos\n",
      "Shoplifting: 40 videos\n",
      "Stealing: 80 videos\n",
      "Vandalism: 40 videos\n",
      "Abuse: 5 videos\n",
      "Arrest: 5 videos\n",
      "Arson: 5 videos\n",
      "Assault: 5 videos\n",
      "Burglary: 10 videos\n",
      "Explosion: 5 videos\n",
      "Fighting: 5 videos\n",
      "NormalVideos: 150 videos\n",
      "RoadAccidents: 15 videos\n",
      "Robbery: 15 videos\n",
      "Shooting: 5 videos\n",
      "Shoplifting: 5 videos\n",
      "Stealing: 10 videos\n",
      "Vandalism: 5 videos\n",
      "Abuse: 5 videos\n",
      "Arrest: 5 videos\n",
      "Arson: 5 videos\n",
      "Assault: 5 videos\n",
      "Burglary: 10 videos\n",
      "Explosion: 5 videos\n",
      "Fighting: 5 videos\n",
      "NormalVideos: 19 videos\n",
      "RoadAccidents: 15 videos\n",
      "Robbery: 15 videos\n",
      "Shooting: 5 videos\n",
      "Shoplifting: 5 videos\n",
      "Stealing: 10 videos\n",
      "Vandalism: 5 videos\n",
      "\n",
      "Total Train videos: 930\n",
      "Total Test videos: 245\n",
      "Total Validation videos: 114\n"
     ]
    }
   ],
   "source": [
    "def count_videos(root_dir):\n",
    "    total = 0\n",
    "    for cls in os.listdir(root_dir):\n",
    "        cls_path = os.path.join(root_dir, cls)\n",
    "        if not os.path.isdir(cls_path):\n",
    "            continue\n",
    "        # Count subdirectories (each subdirectory is a video folder)\n",
    "        num_videos = sum(\n",
    "            1 for file in os.listdir(cls_path)\n",
    "            if file.lower().endswith('.mp4')\n",
    "        )\n",
    "        print(f\"{cls}: {num_videos} videos\")\n",
    "        total += num_videos\n",
    "    return total\n",
    "\n",
    "train_count = count_videos(train_root)\n",
    "test_count = count_videos(test_root)\n",
    "val_count = count_videos(val_root)\n",
    "\n",
    "print(f\"\\nTotal Train videos: {train_count}\")\n",
    "print(f\"Total Test videos: {test_count}\")\n",
    "print(f\"Total Validation videos: {val_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fed0b61",
   "metadata": {},
   "source": [
    "**Process Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00c654f",
   "metadata": {},
   "source": [
    "Map Video Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4544857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = sorted(os.listdir(train_root))\n",
    "num_classes = len(class_labels)\n",
    "label2id = {label: idx for idx, label in enumerate(class_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4f5dac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MapAndPath(root, label2id):\n",
    "    videopaths = []\n",
    "\n",
    "    for clss in sorted(os.listdir(root)):\n",
    "        class_dir = os.path.join(root, clss)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "\n",
    "        for file in os.listdir(class_dir):\n",
    "            if file.lower().endswith('.mp4'):\n",
    "                filepath = os.path.join(class_dir, file)\n",
    "                label_id = label2id[clss]\n",
    "                videopaths.append((filepath, {\"label\": label_id, \"video_path\": filepath}))\n",
    "                \n",
    "    return videopaths\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecf503d",
   "metadata": {},
   "source": [
    "trying to wrap the class to properly allow getting the length and getting items. cv2 was used to try and help some videos being skipped in loading, but its not quite working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c33c80ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SafeLabeledVideoDataset(Dataset):\n",
    "    def __init__(self, labeled_video_paths, transform=None, num_frames=16, frame_size=224):\n",
    "        self.labeled_video_paths = labeled_video_paths\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "        self.frame_size = frame_size\n",
    "        self.length = len(labeled_video_paths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def _load_video_cv2(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(\"Could not open video\")\n",
    "\n",
    "        frames = []\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n",
    "\n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, (self.frame_size, self.frame_size))\n",
    "            frames.append(torch.from_numpy(frame.astype(np.float32) / 255.0).permute(2, 0, 1))  # (C, H, W)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if len(frames) == 0:\n",
    "            raise ValueError(\"No frames extracted\")\n",
    "\n",
    "        video_tensor = torch.stack(frames, dim=1)  # (C, T, H, W)\n",
    "        return video_tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, metadata = self.labeled_video_paths[idx]\n",
    "        try:\n",
    "            video_tensor = self._load_video_cv2(video_path)\n",
    "            print(f\"After load: {video_tensor.shape}\")\n",
    "\n",
    "            if self.transform:\n",
    "                video_dict = {\"video\": video_tensor}\n",
    "                print(f\"Before transform: {video_dict['video'].shape}\")\n",
    "                video_dict = self.transform(video_dict)\n",
    "                print(f\"After transform: {video_dict['video'].shape}\")\n",
    "                video_tensor = video_dict[\"video\"]\n",
    "\n",
    "            return {\n",
    "                \"video\": video_tensor,\n",
    "                \"label\": metadata[\"label\"],\n",
    "                \"video_path\": video_path,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[LOAD FAILED] {video_path} | Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()  # This will show the full error trace\n",
    "            dummy = torch.zeros(3, self.num_frames, self.frame_size, self.frame_size)\n",
    "            return {\"video\": dummy, \"label\": metadata[\"label\"], \"video_path\": video_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c5f2e2",
   "metadata": {},
   "source": [
    "Load Model and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50397355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers.utils as utils\n",
    "utils.is_torch_available = lambda: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0029e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Brandon\\anaconda3\\envs\\ucf_crime_env\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VideoMAEForVideoClassification(\n",
       "  (videomae): VideoMAEModel(\n",
       "    (embeddings): VideoMAEEmbeddings(\n",
       "      (patch_embeddings): VideoMAEPatchEmbeddings(\n",
       "        (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "      )\n",
       "    )\n",
       "    (encoder): VideoMAEEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=14, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"MCG-NJU/videomae-base\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = VideoMAEForVideoClassification.from_pretrained(model_name, num_labels=num_classes, label2id=label2id, id2label=id2label, ignore_mismatched_sizes=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d94063",
   "metadata": {},
   "source": [
    "transforming videos in a similar way to hugging face documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab7dea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    height = width = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    height = image_processor.size[\"height\"]\n",
    "    width = image_processor.size[\"width\"]\n",
    "resize_to = (height, width)\n",
    "\n",
    "num_frames_to_sample = model.config.num_frames\n",
    "sample_rate = 4\n",
    "fps = 30\n",
    "clip_duration = num_frames_to_sample * sample_rate / fps\n",
    "\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    RandomShortSideScale(min_size=256, max_size=320),\n",
    "                    RandomCrop(resize_to),\n",
    "                    RandomHorizontalFlip(p=0.5),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    Resize(resize_to),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa24829c",
   "metadata": {},
   "source": [
    "create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbaad712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 930, Test: 245, Val: 114\n"
     ]
    }
   ],
   "source": [
    "train_paths = MapAndPath(train_root, label2id)\n",
    "val_paths = MapAndPath(val_root, label2id)\n",
    "test_paths = MapAndPath(test_root, label2id)\n",
    "\n",
    "print(f\"Train: {len(train_paths)}, Test: {len(test_paths)}, Val: {len(val_paths)}\")\n",
    "\n",
    "train_dataset = SafeLabeledVideoDataset(\n",
    "    labeled_video_paths=train_paths,\n",
    "    transform=train_transform\n",
    "    \n",
    ")\n",
    "\n",
    "test_dataset = SafeLabeledVideoDataset(\n",
    "    labeled_video_paths=test_paths,\n",
    "    transform=val_transform\n",
    "    \n",
    ")\n",
    "\n",
    "val_dataset = SafeLabeledVideoDataset(\n",
    "    labeled_video_paths=val_paths,\n",
    "    transform=val_transform\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2dcf2d",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a6e8697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=preds, references=eval_pred.label_ids)\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([ex[\"video\"] for ex in examples]) \n",
    "    labels = torch.tensor([ex[\"label\"] for ex in examples])\n",
    "    paths = [ex.get(\"video_path\", \"unknown\") for ex in examples]\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels, \"video_path\": paths}\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 3\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=str(output_dir),\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=10,\n",
    "        learning_rate=5e-5,\n",
    "        warmup_ratio=0.1,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        num_train_epochs=num_epochs,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        push_to_hub=False,\n",
    "        remove_unused_columns=False,\n",
    "        max_steps=(len(train_dataset) // batch_size) * num_epochs,\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=image_processor,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=collate_fn,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb88a6ab",
   "metadata": {},
   "source": [
    "Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46a1b4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/82 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD FAILED] C:\\Users\\Brandon\\Documents\\DM_Project\\ucf_crime_v2\\Test\\Abuse\\Abuse003_x264.mp4 | Error: too many indices for tensor of dimension 4\n",
      "[LOAD FAILED] C:\\Users\\Brandon\\Documents\\DM_Project\\ucf_crime_v2\\Test\\Abuse\\Abuse011_x264.mp4 | Error: too many indices for tensor of dimension 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|          | 1/82 [00:00<00:52,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD FAILED] C:\\Users\\Brandon\\Documents\\DM_Project\\ucf_crime_v2\\Test\\Abuse\\Abuse013_x264.mp4 | Error: too many indices for tensor of dimension 4\n",
      "Batch failed: ['C:\\\\Users\\\\Brandon\\\\Documents\\\\DM_Project\\\\ucf_crime_v2\\\\Test\\\\Abuse\\\\Abuse003_x264.mp4', 'C:\\\\Users\\\\Brandon\\\\Documents\\\\DM_Project\\\\ucf_crime_v2\\\\Test\\\\Abuse\\\\Abuse011_x264.mp4', 'C:\\\\Users\\\\Brandon\\\\Documents\\\\DM_Project\\\\ucf_crime_v2\\\\Test\\\\Abuse\\\\Abuse013_x264.mp4'] | Make sure that the channel dimension of the pixel values match with the one set in the configuration.\n",
      "[LOAD FAILED] C:\\Users\\Brandon\\Documents\\DM_Project\\ucf_crime_v2\\Test\\Abuse\\Abuse039_x264.mp4 | Error: too many indices for tensor of dimension 4\n",
      "[LOAD FAILED] C:\\Users\\Brandon\\Documents\\DM_Project\\ucf_crime_v2\\Test\\Abuse\\Abuse047_x264.mp4 | Error: too many indices for tensor of dimension 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   2%|▏         | 2/82 [00:01<00:48,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD FAILED] C:\\Users\\Brandon\\Documents\\DM_Project\\ucf_crime_v2\\Test\\Arrest\\Arrest013_x264.mp4 | Error: too many indices for tensor of dimension 4\n",
      "Batch failed: ['C:\\\\Users\\\\Brandon\\\\Documents\\\\DM_Project\\\\ucf_crime_v2\\\\Test\\\\Abuse\\\\Abuse039_x264.mp4', 'C:\\\\Users\\\\Brandon\\\\Documents\\\\DM_Project\\\\ucf_crime_v2\\\\Test\\\\Abuse\\\\Abuse047_x264.mp4', 'C:\\\\Users\\\\Brandon\\\\Documents\\\\DM_Project\\\\ucf_crime_v2\\\\Test\\\\Arrest\\\\Arrest013_x264.mp4'] | Make sure that the channel dimension of the pixel values match with the one set in the configuration.\n",
      "[LOAD FAILED] C:\\Users\\Brandon\\Documents\\DM_Project\\ucf_crime_v2\\Test\\Arrest\\Arrest015_x264.mp4 | Error: too many indices for tensor of dimension 4\n",
      "[LOAD FAILED] C:\\Users\\Brandon\\Documents\\DM_Project\\ucf_crime_v2\\Test\\Arrest\\Arrest023_x264.mp4 | Error: too many indices for tensor of dimension 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   4%|▎         | 3/82 [00:01<00:49,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD FAILED] C:\\Users\\Brandon\\Documents\\DM_Project\\ucf_crime_v2\\Test\\Arrest\\Arrest024_x264.mp4 | Error: too many indices for tensor of dimension 4\n",
      "Batch failed: ['C:\\\\Users\\\\Brandon\\\\Documents\\\\DM_Project\\\\ucf_crime_v2\\\\Test\\\\Arrest\\\\Arrest015_x264.mp4', 'C:\\\\Users\\\\Brandon\\\\Documents\\\\DM_Project\\\\ucf_crime_v2\\\\Test\\\\Arrest\\\\Arrest023_x264.mp4', 'C:\\\\Users\\\\Brandon\\\\Documents\\\\DM_Project\\\\ucf_crime_v2\\\\Test\\\\Arrest\\\\Arrest024_x264.mp4'] | Make sure that the channel dimension of the pixel values match with the one set in the configuration.\n",
      "[LOAD FAILED] C:\\Users\\Brandon\\Documents\\DM_Project\\ucf_crime_v2\\Test\\Arrest\\Arrest046_x264.mp4 | Error: too many indices for tensor of dimension 4\n",
      "[LOAD FAILED] C:\\Users\\Brandon\\Documents\\DM_Project\\ucf_crime_v2\\Test\\Arson\\Arson012_x264.mp4 | Error: too many indices for tensor of dimension 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   5%|▍         | 4/82 [00:02<00:44,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD FAILED] C:\\Users\\Brandon\\Documents\\DM_Project\\ucf_crime_v2\\Test\\Arson\\Arson021_x264.mp4 | Error: too many indices for tensor of dimension 4\n",
      "Batch failed: ['C:\\\\Users\\\\Brandon\\\\Documents\\\\DM_Project\\\\ucf_crime_v2\\\\Test\\\\Arrest\\\\Arrest046_x264.mp4', 'C:\\\\Users\\\\Brandon\\\\Documents\\\\DM_Project\\\\ucf_crime_v2\\\\Test\\\\Arson\\\\Arson012_x264.mp4', 'C:\\\\Users\\\\Brandon\\\\Documents\\\\DM_Project\\\\ucf_crime_v2\\\\Test\\\\Arson\\\\Arson021_x264.mp4'] | Make sure that the channel dimension of the pixel values match with the one set in the configuration.\n",
      "[LOAD FAILED] C:\\Users\\Brandon\\Documents\\DM_Project\\ucf_crime_v2\\Test\\Arson\\Arson024_x264.mp4 | Error: too many indices for tensor of dimension 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   6%|▌         | 5/82 [00:02<00:39,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD FAILED] C:\\Users\\Brandon\\Documents\\DM_Project\\ucf_crime_v2\\Test\\Arson\\Arson035_x264.mp4 | Error: too many indices for tensor of dimension 4\n",
      "[LOAD FAILED] C:\\Users\\Brandon\\Documents\\DM_Project\\ucf_crime_v2\\Test\\Arson\\Arson048_x264.mp4 | Error: too many indices for tensor of dimension 4\n",
      "Batch failed: ['C:\\\\Users\\\\Brandon\\\\Documents\\\\DM_Project\\\\ucf_crime_v2\\\\Test\\\\Arson\\\\Arson024_x264.mp4', 'C:\\\\Users\\\\Brandon\\\\Documents\\\\DM_Project\\\\ucf_crime_v2\\\\Test\\\\Arson\\\\Arson035_x264.mp4', 'C:\\\\Users\\\\Brandon\\\\Documents\\\\DM_Project\\\\ucf_crime_v2\\\\Test\\\\Arson\\\\Arson048_x264.mp4'] | Make sure that the channel dimension of the pixel values match with the one set in the configuration.\n",
      "[LOAD FAILED] C:\\Users\\Brandon\\Documents\\DM_Project\\ucf_crime_v2\\Test\\Assault\\Assault001_x264.mp4 | Error: too many indices for tensor of dimension 4\n",
      "[LOAD FAILED] C:\\Users\\Brandon\\Documents\\DM_Project\\ucf_crime_v2\\Test\\Assault\\Assault010_x264.mp4 | Error: too many indices for tensor of dimension 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   7%|▋         | 6/82 [00:03<00:39,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD FAILED] C:\\Users\\Brandon\\Documents\\DM_Project\\ucf_crime_v2\\Test\\Assault\\Assault015_x264.mp4 | Error: too many indices for tensor of dimension 4\n",
      "Batch failed: ['C:\\\\Users\\\\Brandon\\\\Documents\\\\DM_Project\\\\ucf_crime_v2\\\\Test\\\\Assault\\\\Assault001_x264.mp4', 'C:\\\\Users\\\\Brandon\\\\Documents\\\\DM_Project\\\\ucf_crime_v2\\\\Test\\\\Assault\\\\Assault010_x264.mp4', 'C:\\\\Users\\\\Brandon\\\\Documents\\\\DM_Project\\\\ucf_crime_v2\\\\Test\\\\Assault\\\\Assault015_x264.mp4'] | Make sure that the channel dimension of the pixel values match with the one set in the configuration.\n",
      "[LOAD FAILED] C:\\Users\\Brandon\\Documents\\DM_Project\\ucf_crime_v2\\Test\\Assault\\Assault022_x264.mp4 | Error: too many indices for tensor of dimension 4\n",
      "[LOAD FAILED] C:\\Users\\Brandon\\Documents\\DM_Project\\ucf_crime_v2\\Test\\Assault\\Assault033_x264.mp4 | Error: too many indices for tensor of dimension 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   9%|▊         | 7/82 [00:03<00:42,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD FAILED] C:\\Users\\Brandon\\Documents\\DM_Project\\ucf_crime_v2\\Test\\Burglary\\Burglary002_x264.mp4 | Error: too many indices for tensor of dimension 4\n",
      "Batch failed: ['C:\\\\Users\\\\Brandon\\\\Documents\\\\DM_Project\\\\ucf_crime_v2\\\\Test\\\\Assault\\\\Assault022_x264.mp4', 'C:\\\\Users\\\\Brandon\\\\Documents\\\\DM_Project\\\\ucf_crime_v2\\\\Test\\\\Assault\\\\Assault033_x264.mp4', 'C:\\\\Users\\\\Brandon\\\\Documents\\\\DM_Project\\\\ucf_crime_v2\\\\Test\\\\Burglary\\\\Burglary002_x264.mp4'] | Make sure that the channel dimension of the pixel values match with the one set in the configuration.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 13\u001b[0m\n\u001b[0;32m      4\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m      5\u001b[0m     test_dataset,\n\u001b[0;32m      6\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(test_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m             inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n",
      "File \u001b[1;32mc:\\Users\\Brandon\\anaconda3\\envs\\ucf_crime_env\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Brandon\\anaconda3\\envs\\ucf_crime_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Brandon\\anaconda3\\envs\\ucf_crime_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Brandon\\anaconda3\\envs\\ucf_crime_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Brandon\\anaconda3\\envs\\ucf_crime_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[7], line 46\u001b[0m, in \u001b[0;36mSafeLabeledVideoDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     44\u001b[0m video_path, metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabeled_video_paths[idx]\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 46\u001b[0m     video_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_video_cv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m     49\u001b[0m         video_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(video_tensor)\n",
      "Cell \u001b[1;32mIn[7], line 27\u001b[0m, in \u001b[0;36mSafeLabeledVideoDataset._load_video_cv2\u001b[1;34m(self, video_path)\u001b[0m\n\u001b[0;32m     24\u001b[0m frame_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, total_frames \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_frames, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m frame_indices:\n\u001b[1;32m---> 27\u001b[0m     \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCAP_PROP_POS_FRAMES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds, all_labels, failed_paths = [], [], []\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        try:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k not in [\"labels\", \"video_path\"]}\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(**inputs)\n",
    "            preds = outputs.logits.argmax(dim=1).cpu().tolist()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "        except Exception as e:\n",
    "            print(f\"Batch failed: {batch.get('video_path', [])} | {e}\")\n",
    "            failed_paths.extend(batch.get(\"video_path\", []))\n",
    "            # Add dummy preds to keep length\n",
    "            all_preds.extend([0] * len(labels))\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "print(f\"Failed videos: {len(failed_paths)}\")\n",
    "if failed_paths:\n",
    "    print(\"First 5 failed paths:\", failed_paths[:5])\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=np.arange(num_classes))\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=class_labels,\n",
    "    yticklabels=class_labels,\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Test Set Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "cm_path = output_dir / \"confusion_matrix.png\"\n",
    "plt.savefig(cm_path)\n",
    "plt.close()\n",
    "print(f\"Confusion matrix saved to {cm_path}\")\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(\n",
    "    all_labels, all_preds, target_names=class_labels, digits=4\n",
    ")\n",
    "report_path = output_dir / \"classification_report.txt\"\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(report)\n",
    "print(\"\\nClassification report:\\n\", report)\n",
    "print(f\"Report saved to {report_path}\")\n",
    "\n",
    "acc = np.trace(cm) / np.sum(cm)\n",
    "print(f\"\\nOverall Test Accuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucf_crime_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
