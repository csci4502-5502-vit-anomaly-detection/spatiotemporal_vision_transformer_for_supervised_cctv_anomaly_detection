{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b14e77f",
   "metadata": {},
   "source": [
    "# Experiment 2 - Stage 1: Binary Crime Classification (OpenCV Data Ingestion)\n",
    "\n",
    "## Overview\n",
    "This notebook uses **Experiment 1's data ingestion approach** (direct MP4 loading with OpenCV) while keeping the original Experiment 2 model architecture.\n",
    "\n",
    "### Changes from Original:\n",
    "- \u2705 **Data Ingestion**: MP4 videos loaded directly using OpenCV (instead of pre-extracted frames)\n",
    "- \u2705 **On-the-fly frame extraction**: No preprocessing required\n",
    "- \u26a0\ufe0f **Model**: Original simple 3D CNN (no attention mechanism)\n",
    "- \u26a0\ufe0f **Training**: Basic training loop (no gradient clipping or advanced features)\n",
    "\n",
    "For advanced features (attention mechanism, gradient clipping), see: `experiment_2_enhanced.ipynb`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc1ba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72348811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCFCrimeBinaryDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that loads videos directly from MP4 files using OpenCV\n",
    "    (matching Experiment 1's data ingestion approach)\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, clip_len=16, transform=None, frame_size=112):\n",
    "        self.root_dir = root_dir\n",
    "        self.clip_len = clip_len\n",
    "        self.transform = transform\n",
    "        self.frame_size = frame_size\n",
    "\n",
    "        self.samples = []\n",
    "        self._prepare_samples()\n",
    "\n",
    "    def _prepare_samples(self):\n",
    "        \"\"\"Scan directory for MP4 video files\"\"\"\n",
    "        for category in os.listdir(self.root_dir):\n",
    "            category_path = os.path.join(self.root_dir, category)\n",
    "            if not os.path.isdir(category_path):\n",
    "                continue\n",
    "\n",
    "            label = 0 if category.lower() == \"normalvideos\" else 1  # normal=0, crime=1\n",
    "            \n",
    "            # Look for .mp4 files (not directories)\n",
    "            for file in os.listdir(category_path):\n",
    "                if file.lower().endswith('.mp4'):\n",
    "                    video_path = os.path.join(category_path, file)\n",
    "                    self.samples.append((video_path, label))\n",
    "\n",
    "    def _load_video_cv2(self, video_path):\n",
    "        \"\"\"Load video using OpenCV and extract frames\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video: {video_path}\")\n",
    "\n",
    "        frames = []\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # Sample frames uniformly across the video\n",
    "        frame_indices = np.linspace(0, total_frames - 1, self.clip_len, dtype=int)\n",
    "\n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            # Convert BGR to RGB\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # Resize frame\n",
    "            frame = cv2.resize(frame, (self.frame_size, self.frame_size))\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if len(frames) == 0:\n",
    "            raise ValueError(f\"No frames extracted from: {video_path}\")\n",
    "\n",
    "        # Pad with last frame if needed\n",
    "        while len(frames) < self.clip_len:\n",
    "            frames.append(frames[-1])\n",
    "\n",
    "        return frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, label = self.samples[idx]\n",
    "        \n",
    "        try:\n",
    "            # Load frames using OpenCV\n",
    "            frames = self._load_video_cv2(video_path)\n",
    "            \n",
    "            # Convert to tensors\n",
    "            imgs = []\n",
    "            for frame in frames:\n",
    "                # Convert numpy array to PIL Image for transforms\n",
    "                frame_pil = Image.fromarray(frame)\n",
    "                if self.transform:\n",
    "                    frame_tensor = self.transform(frame_pil)\n",
    "                else:\n",
    "                    frame_tensor = transforms.ToTensor()(frame_pil)\n",
    "                imgs.append(frame_tensor)\n",
    "            \n",
    "            # Stack as (C, T, H, W)\n",
    "            clip_tensor = torch.stack(imgs, dim=1)\n",
    "            return clip_tensor, torch.tensor(label, dtype=torch.long), video_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[LOAD FAILED] {video_path} | Error: {e}\")\n",
    "            # Return dummy tensor\n",
    "            dummy = torch.zeros(3, self.clip_len, self.frame_size, self.frame_size)\n",
    "            return dummy, torch.tensor(label, dtype=torch.long), video_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7fa036",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrimeDetector(nn.Module):\n",
    "    \"\"\"Original simple 3D CNN for binary crime detection\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(3, 32, (1,3,3), padding=(0,1,1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(32, 32, (3,1,1), padding=(1,0,0)),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d((1,2,2)),\n",
    "\n",
    "            nn.Conv3d(32, 64, (1,3,3), padding=(0,1,1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(64, 64, (3,1,1), padding=(1,0,0)),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d((2,2,2))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool3d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c9600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train clips: 1610, Test clips: 290\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CLIP_LEN = 16\n",
    "FRAME_SIZE = 112\n",
    "\n",
    "# Transform pipeline (resize is now handled in dataset via OpenCV)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# UPDATE THESE PATHS TO YOUR VIDEO DIRECTORIES (should contain .mp4 files, not frame directories)\n",
    "train_dir = r\"C:\\Users\\rayaa\\Downloads\\ucf_crime_v2\\Train\"\n",
    "test_dir  = r\"C:\\Users\\rayaa\\Downloads\\ucf_crime_v2\\Test\"\n",
    "\n",
    "train_data = UCFCrimeBinaryDataset(train_dir, clip_len=CLIP_LEN, transform=transform, frame_size=FRAME_SIZE)\n",
    "test_data  = UCFCrimeBinaryDataset(test_dir, clip_len=CLIP_LEN, transform=transform, frame_size=FRAME_SIZE)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=4, shuffle=True, num_workers=0)\n",
    "test_loader  = DataLoader(test_data, batch_size=2, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train clips: {len(train_data)}, Test clips: {len(test_data)}\")\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793538bf",
   "metadata": {},
   "source": [
    "### Basic Training Loop\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc1ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryCrimeDetector().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for clips, labels, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        clips, labels = clips.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(clips)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# save checkpoint\n",
    "output_dir = r\"C:\\Users\\rayaa\\Downloads\\ucf_crime_v2\\checkpoints\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, \"binary_stage1_opencv_simple.pt\"))\n",
    "print(f\"\\n\u2713 Model saved to: {output_dir}/binary_stage1_opencv_simple.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668a5455",
   "metadata": {},
   "source": [
    "### Simple Inference\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2063c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/403 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 403/403 [09:43<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.6681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 403/403 [03:52<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.6361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 403/403 [03:41<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.6067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 403/403 [03:32<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.6061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 403/403 [03:34<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.5924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "anomaly_dir = \"./stage1_output/anomaly_clips\"\n",
    "os.makedirs(anomaly_dir, exist_ok=True)\n",
    "anomaly_records = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for clips, labels, paths in tqdm(test_loader, desc=\"Stage 1 Inference\"):\n",
    "        clips = clips.to(DEVICE)\n",
    "        outputs = model(clips)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        preds = probs.argmax(dim=1).cpu().numpy()\n",
    "        probs = probs[:,1].cpu().numpy()  # prob of 'crime'\n",
    "\n",
    "        for i, pred in enumerate(preds):\n",
    "            if pred == 1:\n",
    "                anomaly_records.append({\n",
    "                    \"video_path\": paths[i],\n",
    "                    \"confidence\": float(probs[i])\n",
    "                })\n",
    "\n",
    "with open(\"./stage1_output/anomalies.json\", \"w\") as f:\n",
    "    json.dump(anomaly_records, f, indent=2)\n",
    "\n",
    "print(f\"Stage 1 complete \u2014 {len(anomaly_records)} crime clips detected\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda12_8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}