{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b14e77f",
   "metadata": {},
   "source": [
    "# Experiment 2 - Stage 1: Binary Crime Classification with Attention\n",
    "\n",
    "## Overview\n",
    "This notebook implements Stage 1 of Experiment 2, which performs binary classification (Normal vs. Crime) on UCF Crime videos.\n",
    "\n",
    "### Key Features:\n",
    "1. **Improved Data Ingestion**: Uses Experiment 1's approach - loading videos directly from MP4 files using OpenCV\n",
    "2. **Attention Mechanism**: Identifies which frames are most important for crime detection\n",
    "3. **Gradient Clipping**: Keeps weights stable between 0 and 1\n",
    "4. **High-Attention Frame Extraction**: Prepares data for Stage 2 by identifying relevant frames\n",
    "\n",
    "### Stage 1 Goals:\n",
    "- Train a binary classifier (Normal=0, Crime=1)\n",
    "- Extract attention weights to identify crime-relevant frames\n",
    "- Save high-attention data for Stage 2 training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc1ba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72348811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCFCrimeBinaryDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that loads videos directly from MP4 files using OpenCV\n",
    "    (matching Experiment 1's data ingestion approach)\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, clip_len=16, transform=None, frame_size=112):\n",
    "        self.root_dir = root_dir\n",
    "        self.clip_len = clip_len\n",
    "        self.transform = transform\n",
    "        self.frame_size = frame_size\n",
    "\n",
    "        self.samples = []\n",
    "        self._prepare_samples()\n",
    "\n",
    "    def _prepare_samples(self):\n",
    "        \"\"\"Scan directory for MP4 video files\"\"\"\n",
    "        for category in os.listdir(self.root_dir):\n",
    "            category_path = os.path.join(self.root_dir, category)\n",
    "            if not os.path.isdir(category_path):\n",
    "                continue\n",
    "\n",
    "            label = 0 if category.lower() == \"normalvideos\" else 1  # normal=0, crime=1\n",
    "            \n",
    "            # Look for .mp4 files (not directories)\n",
    "            for file in os.listdir(category_path):\n",
    "                if file.lower().endswith('.mp4'):\n",
    "                    video_path = os.path.join(category_path, file)\n",
    "                    self.samples.append((video_path, label))\n",
    "\n",
    "    def _load_video_cv2(self, video_path):\n",
    "        \"\"\"Load video using OpenCV and extract frames\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video: {video_path}\")\n",
    "\n",
    "        frames = []\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # Sample frames uniformly across the video\n",
    "        frame_indices = np.linspace(0, total_frames - 1, self.clip_len, dtype=int)\n",
    "\n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            # Convert BGR to RGB\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # Resize frame\n",
    "            frame = cv2.resize(frame, (self.frame_size, self.frame_size))\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if len(frames) == 0:\n",
    "            raise ValueError(f\"No frames extracted from: {video_path}\")\n",
    "\n",
    "        # Pad with last frame if needed\n",
    "        while len(frames) < self.clip_len:\n",
    "            frames.append(frames[-1])\n",
    "\n",
    "        return frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, label = self.samples[idx]\n",
    "        \n",
    "        try:\n",
    "            # Load frames using OpenCV\n",
    "            frames = self._load_video_cv2(video_path)\n",
    "            \n",
    "            # Convert to tensors\n",
    "            imgs = []\n",
    "            for frame in frames:\n",
    "                # Convert numpy array to PIL Image for transforms\n",
    "                frame_pil = Image.fromarray(frame)\n",
    "                if self.transform:\n",
    "                    frame_tensor = self.transform(frame_pil)\n",
    "                else:\n",
    "                    frame_tensor = transforms.ToTensor()(frame_pil)\n",
    "                imgs.append(frame_tensor)\n",
    "            \n",
    "            # Stack as (C, T, H, W)\n",
    "            clip_tensor = torch.stack(imgs, dim=1)\n",
    "            return clip_tensor, torch.tensor(label, dtype=torch.long), video_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[LOAD FAILED] {video_path} | Error: {e}\")\n",
    "            # Return dummy tensor\n",
    "            dummy = torch.zeros(3, self.clip_len, self.frame_size, self.frame_size)\n",
    "            return dummy, torch.tensor(label, dtype=torch.long), video_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7fa036",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrimeDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    Binary crime detector with attention mechanism for frame importance\n",
    "    Attention weights help identify which frames are most relevant for crime detection\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(3, 32, (1,3,3), padding=(0,1,1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(32, 32, (3,1,1), padding=(1,0,0)),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d((1,2,2)),\n",
    "\n",
    "            nn.Conv3d(32, 64, (1,3,3), padding=(0,1,1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(64, 64, (3,1,1), padding=(1,0,0)),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d((2,2,2))\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism to identify important frames\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv3d(64, 32, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(32, 1, kernel_size=1),\n",
    "            nn.Sigmoid()  # Attention weights between 0 and 1\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool3d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input tensor (B, C, T, H, W)\n",
    "            return_attention: if True, return attention weights along with predictions\n",
    "        \n",
    "        Returns:\n",
    "            logits: classification logits (B, 2)\n",
    "            attention_weights (optional): frame importance weights (B, T)\n",
    "        \"\"\"\n",
    "        features = self.features(x)\n",
    "        \n",
    "        # Compute attention weights for each frame\n",
    "        attention_map = self.attention(features)  # (B, 1, T', H', W')\n",
    "        \n",
    "        # Global average pooling over spatial dimensions to get per-frame weights\n",
    "        attention_weights = attention_map.mean(dim=[3, 4])  # (B, 1, T')\n",
    "        attention_weights = attention_weights.squeeze(1)  # (B, T')\n",
    "        \n",
    "        # Apply attention to features\n",
    "        weighted_features = features * attention_map\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(weighted_features)\n",
    "        \n",
    "        if return_attention:\n",
    "            return logits, attention_weights\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c9600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train clips: 1610, Test clips: 290\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CLIP_LEN = 16\n",
    "FRAME_SIZE = 112\n",
    "\n",
    "# Transform pipeline (resize is now handled in dataset via OpenCV)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# UPDATE THESE PATHS TO YOUR VIDEO DIRECTORIES (should contain .mp4 files, not frame directories)\n",
    "train_dir = r\"C:\\Users\\rayaa\\Downloads\\ucf_crime_v2\\Train\"\n",
    "test_dir  = r\"C:\\Users\\rayaa\\Downloads\\ucf_crime_v2\\Test\"\n",
    "\n",
    "train_data = UCFCrimeBinaryDataset(train_dir, clip_len=CLIP_LEN, transform=transform, frame_size=FRAME_SIZE)\n",
    "test_data  = UCFCrimeBinaryDataset(test_dir, clip_len=CLIP_LEN, transform=transform, frame_size=FRAME_SIZE)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=4, shuffle=True, num_workers=0)\n",
    "test_loader  = DataLoader(test_data, batch_size=2, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train clips: {len(train_data)}, Test clips: {len(test_data)}\")\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793538bf",
   "metadata": {},
   "source": [
    "### Verify Data Distribution\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc1ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_videos_per_class(dataset):\n",
    "    \"\"\"Count videos per class for verification\"\"\"\n",
    "    from collections import Counter\n",
    "    labels_count = Counter([label for _, label in dataset.samples])\n",
    "    \n",
    "    print(\"\\nClass Distribution:\")\n",
    "    print(f\"  Normal (0): {labels_count[0]} videos\")\n",
    "    print(f\"  Crime (1): {labels_count[1]} videos\")\n",
    "    print(f\"  Total: {len(dataset)} videos\")\n",
    "    return labels_count\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING SET\")\n",
    "print(\"=\" * 60)\n",
    "train_dist = count_videos_per_class(train_data)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEST SET\")\n",
    "print(\"=\" * 60)\n",
    "test_dist = count_videos_per_class(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668a5455",
   "metadata": {},
   "source": [
    "### Training with Gradient Clipping & Attention\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2063c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/403 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 403/403 [09:43<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.6681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 403/403 [03:52<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.6361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 403/403 [03:41<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.6067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 403/403 [03:32<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.6061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 403/403 [03:34<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.5924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = BinaryCrimeDetector().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n",
    "\n",
    "EPOCHS = 5\n",
    "GRADIENT_CLIP_VALUE = 1.0  # Gradient clipping for stable training\n",
    "\n",
    "print(f\"\\nStarting training with attention mechanism...\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Gradient clipping: {GRADIENT_CLIP_VALUE}\")\n",
    "print(f\"Device: {DEVICE}\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for clips, labels, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        clips, labels = clips.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with attention\n",
    "        outputs, attention_weights = model(clips, return_attention=True)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping (keeps weights stable between 0 and 1)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP_VALUE)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    # Scheduler step\n",
    "    scheduler.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | Accuracy: {accuracy:.2f}% | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "# save checkpoint\n",
    "output_dir = r\"C:\\Users\\rayaa\\Downloads\\ucf_crime_v2\\checkpoints\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "checkpoint_path = os.path.join(output_dir, \"binary_stage1_with_attention.pt\")\n",
    "torch.save({\n",
    "    'epoch': EPOCHS,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "}, checkpoint_path)\n",
    "print(f\"\\n✓ Model checkpoint saved to: {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6c2b13",
   "metadata": {},
   "source": [
    "### Stage 1 Inference - Extract Anomaly Clips with High Attention\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b2531d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1 Inference: 100%|██████████| 145/145 [10:22<00:00,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 complete — 153 crime clips saved to ./stage1_output/anomaly_clips\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "anomaly_dir = \"./stage1_output/anomaly_clips\"\n",
    "attention_output_dir = \"./stage1_output/attention_data\"\n",
    "os.makedirs(anomaly_dir, exist_ok=True)\n",
    "os.makedirs(attention_output_dir, exist_ok=True)\n",
    "\n",
    "anomaly_records = []\n",
    "all_attention_data = []\n",
    "\n",
    "print(\"\\nRunning Stage 1 inference with attention extraction...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for clips, labels, paths in tqdm(test_loader, desc=\"Stage 1 Inference\"):\n",
    "        clips = clips.to(DEVICE)\n",
    "        \n",
    "        # Get predictions and attention weights\n",
    "        outputs, attention_weights = model(clips, return_attention=True)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        preds = probs.argmax(dim=1).cpu().numpy()\n",
    "        crime_probs = probs[:,1].cpu().numpy()  # prob of 'crime'\n",
    "        \n",
    "        # Process each clip in batch\n",
    "        for i, pred in enumerate(preds):\n",
    "            if pred == 1:  # Crime detected\n",
    "                video_path = paths[i]\n",
    "                confidence = float(crime_probs[i])\n",
    "                att_weights = attention_weights[i].cpu().numpy()\n",
    "                \n",
    "                # Save video information\n",
    "                anomaly_records.append({\n",
    "                    \"video_path\": video_path,\n",
    "                    \"confidence\": confidence,\n",
    "                    \"attention_weights\": att_weights.tolist(),\n",
    "                    \"top_attention_indices\": np.argsort(att_weights)[-5:].tolist()  # Top 5 frames\n",
    "                })\n",
    "                \n",
    "                # Store attention data for Stage 2\n",
    "                all_attention_data.append({\n",
    "                    \"video_path\": video_path,\n",
    "                    \"confidence\": confidence,\n",
    "                    \"attention_weights\": att_weights,\n",
    "                    \"clip_tensor\": clips[i].cpu()  # Save for potential Stage 2 use\n",
    "                })\n",
    "\n",
    "# Save anomaly records as JSON\n",
    "anomalies_json_path = \"./stage1_output/anomalies.json\"\n",
    "with open(anomalies_json_path, \"w\") as f:\n",
    "    json.dump(anomaly_records, f, indent=2)\n",
    "\n",
    "# Save attention data for Stage 2\n",
    "attention_data_path = os.path.join(attention_output_dir, \"stage1_attention_data.pt\")\n",
    "torch.save(all_attention_data, attention_data_path)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"✓ Stage 1 Complete!\")\n",
    "print(f\"  - Detected {len(anomaly_records)} crime clips\")\n",
    "print(f\"  - Anomaly records saved to: {anomalies_json_path}\")\n",
    "print(f\"  - Attention data saved to: {attention_data_path}\")\n",
    "print(f\"  - Ready for Stage 2 training\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831b5b0f",
   "metadata": {},
   "source": [
    "### (Optional) Visualize Attention Weights\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb274e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_attention_examples(num_examples=3):\n",
    "    \"\"\"Visualize attention weights for a few crime examples\"\"\"\n",
    "    if len(anomaly_records) == 0:\n",
    "        print(\"No anomaly records found. Run inference first.\")\n",
    "        return\n",
    "    \n",
    "    # Select random examples\n",
    "    import random\n",
    "    examples = random.sample(anomaly_records, min(num_examples, len(anomaly_records)))\n",
    "    \n",
    "    fig, axes = plt.subplots(num_examples, 1, figsize=(12, 3*num_examples))\n",
    "    if num_examples == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, example in enumerate(examples):\n",
    "        video_name = os.path.basename(example['video_path'])\n",
    "        attention = np.array(example['attention_weights'])\n",
    "        confidence = example['confidence']\n",
    "        \n",
    "        axes[idx].bar(range(len(attention)), attention, color='crimson', alpha=0.7)\n",
    "        axes[idx].set_xlabel('Frame Index')\n",
    "        axes[idx].set_ylabel('Attention Weight')\n",
    "        axes[idx].set_title(f'{video_name} | Confidence: {confidence:.3f}')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Highlight top frames\n",
    "        top_indices = example['top_attention_indices']\n",
    "        for top_idx in top_indices:\n",
    "            if top_idx < len(attention):\n",
    "                axes[idx].bar(top_idx, attention[top_idx], color='darkred', alpha=1.0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./stage1_output/attention_visualization.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"\\n✓ Attention visualization saved to: ./stage1_output/attention_visualization.png\")\n",
    "\n",
    "# Visualize if anomalies were detected\n",
    "if len(anomaly_records) > 0:\n",
    "    visualize_attention_examples(num_examples=min(3, len(anomaly_records)))\n",
    "else:\n",
    "    print(\"No anomalies detected to visualize.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda12_8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
